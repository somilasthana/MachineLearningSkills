{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somilasthana/MachineLearningSkills/blob/master/GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2muno580k10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "GMM: A Gaussian mixture model is a probabilistic model that assumes all the \n",
        "data points are generated from a mixture of a finite number of Gaussian \n",
        "distributions with unknown parameters.\n",
        "\n",
        "One can think of mixture models as generalizing k-means clustering to \n",
        "incorporate information about the covariance structure of the data as well as \n",
        "the centers of the latent Gaussians.\n",
        "\n",
        "The GaussianMixture object implements the expectation-maximization (EM) \n",
        "algorithm for fitting mixture-of-Gaussian models. It can also draw confidence \n",
        "ellipsoids for multivariate models, and compute the \n",
        "Bayesian Information Criterion to assess the number of clusters in the data.\n",
        "\n",
        "+ : It is the fastest algorithm for learning mixture models\n",
        "    It will not bias the means towards zero, or bias the cluster sizes to have specific structures\n",
        "    \n",
        "\n",
        "- : When one has insufficiently many points per mixture, estimating the covariance \n",
        "    matrices becomes difficult, and the algorithm is known to diverge\n",
        "    \n",
        "    This algorithm will always use all the components it has access to\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_opr39P0nHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The main difficulty in learning Gaussian mixture models from unlabeled data \n",
        "is that it is one usually doesnâ€™t know which points came from which latent \n",
        "component (if one has access to this information it gets very easy to fit a \n",
        "separate Gaussian distribution to each set of points).\n",
        "\n",
        "EM Algo:\n",
        "\n",
        "First one assumes random components and computes for each point a probability of \n",
        "being generated by each component of the model.\n",
        "Then, one tweaks the parameters to maximize the likelihood of the data given those assignments.\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3DUO0QK0qJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNzdCkW90v4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X=iris.data\n",
        "y=iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePfW669M1BlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X[:, :2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wghxAM1GoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZYXAWS1HT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='viridis') #  Real"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWbcUFp31JRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import mixture\n",
        "gmm =  mixture.GaussianMixture(n_components=3, covariance_type='full').fit(X)\n",
        "labels = gmm.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis') #  GMM Picture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wba1Q-OB1MAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}