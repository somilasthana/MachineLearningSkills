{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch-Linear-Dirichlet-Allocation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somilasthana/MachineLearningSkills/blob/master/Scratch_Linear_Dirichlet_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpVPgQQDpnr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Generative model\n",
        "\n",
        "Theory:\n",
        "LDA is based on the idea that words often have strong semantic relationships to \n",
        "certain topics, and so topics in a given document will consist of a group of similar words.\n",
        "\n",
        "LDA requires us to pick the number of topics for it to discover, \n",
        "LDA assumes that a document is a mixture of a set of latent (unknown) topics, \n",
        "and each topic is another mixture of words.\n",
        "\n",
        "Outputs the words in the text corpus (a set of documents) that frequently occur \n",
        "together within the topic.\n",
        "\n",
        "LDA assumes the documents are generated through some statistical process.\n",
        "Given a document d is a text corpus D, d is generated as\n",
        "\n",
        "1. Number of words in document d, represented by N_d, is drawn from poisson distribution.\n",
        "2. The mixture of topics in document d, represented by theta_d, is drawn from dirichlet distribution.\n",
        "3. Assign each word w_i a topic z_i, in a way so that it is consistent with the document-topic distribution in 2)\n",
        "4. Now that we know the topic z_i of each word w_i. from the topic-word distribution\n",
        "\n",
        "LDA assumes a document is a mixture of topics, where the topics are drawn from \n",
        "the topic-document distribution, and topics consist of words, where the words \n",
        "are drawn from the topic-word distribution.\n",
        "\n",
        "\n",
        "Interested in finding the distribution of topics for each document, and the \n",
        "distribution of words for each topic.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cirFkp_pris4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from __future__ import division\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkeTO0fXuw9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hammadshaikhha/Math-of-Machine-Learning-Course-by-Siraj/master/Latent%20Dirichlet%20Allocation/YoutubeCommentsSpam.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oDdoGRvu05y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4647b40c-9e6a-4390-e5c6-c5269fba5181"
      },
      "source": [
        "comments = pd.read_csv(\"/content/YoutubeCommentsSpam.csv\")\n",
        "comments.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>commentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>+447935454150 lovely girl talk to me xxx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I always end up coming back to this song&lt;br /&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello I am from Palastine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         commentText\n",
              "0           +447935454150 lovely girl talk to me xxx\n",
              "1     I always end up coming back to this song<br />\n",
              "2  my sister just received over 6,500 new <a rel=...\n",
              "3                                               Cool\n",
              "4                          Hello I am from Palastine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARJesrtKw4Vf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "08ddaff0-22ce-4105-a021-745ca76b89c1"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-wonZnKvEkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81ec542c-9eaa-4e51-ac89-0f3d0c5d46cb"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "eng_stop = [str(word) for word in stopwords.words('english')]\n",
        "eng_stop[:10]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShKMiTKRwIem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "# Text data to itterate over\n",
        "text_data = [line for line in comments[\"commentText\"] if line != '']\n",
        "\n",
        "for line in range(len(text_data)):\n",
        "  raw_lower = text_data[line].lower()\n",
        "  line_token = tokenizer.tokenize(raw_lower)\n",
        "  clean_token = [re.sub(r'[^a-zA-Z]','', word) for word in line_token]\n",
        "  stop_token = [word for word in clean_token if not word in eng_stop if word != '']\n",
        "  stem_token = [str(p_stemmer.stem(word)) for word in stop_token]\n",
        "  text_data[line] = stem_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN4IqOMB2Uxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "456c2b78-d96b-4999-81fa-c5fda280ab51"
      },
      "source": [
        "text_data[0:2]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['love', 'girl', 'talk', 'xxx'],\n",
              " ['alway', 'end', 'come', 'back', 'song', 'br']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_oqI1fI2XfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee58eb86-5786-4ddb-a8c9-d10ac2be99e2"
      },
      "source": [
        "words_list = [words for sublist in text_data for words in sublist]\n",
        "vocab_total = set(word_list)\n",
        "list(vocab_total)[1:7]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['officialpsi', 'straight', 'enjoy', 'turn', 'georg', 'gjvinpuemo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2O5d6AB23fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert each comment into a vector by replacing the words by their unique ID\n",
        "\n",
        "text_ID = []\n",
        "\n",
        "for line in range(len(text_data)):\n",
        "  comment_vector = [ list(vocab_total).index(word)  for word in text_data[line]]\n",
        "  text_ID.append(comment_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyp1uOYW4GIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5a5e7e41-cf97-4886-ac83-afdafc9974f9"
      },
      "source": [
        "text_data[1], text_ID[1]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['alway', 'end', 'come', 'back', 'song', 'br'],\n",
              " [1355, 197, 640, 414, 3249, 1990])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95JK1cVS4NQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize hyperparameters in LDA\n",
        "\n",
        "# Dirichlet parameters\n",
        "# Alpha is the parameter for the prior topic distribution within documents\n",
        "alpha = 0.2\n",
        "\n",
        "# Beta is the parameter for the prior topic distribution within documents\n",
        "beta = 0.001\n",
        "\n",
        "# Text corpus itterations\n",
        "corpus_itter = 200\n",
        "\n",
        "# Number of topics\n",
        "K = 2\n",
        "\n",
        "# Vocabulary size\n",
        "V = len(vocab_total)\n",
        "\n",
        "# Number of Documents\n",
        "D = len(text_ID)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aI4flNJ55jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For practical implementation, we will generate the following three count matrices:\n",
        "# 1) Word-Topic count matrix, 2) Topic-Document assignment matrix, 3) Document-Topic count matrix\n",
        "\n",
        "\n",
        "word_topic_count = np.zeros((K,V))\n",
        "topic_doc_assign = [np.zeros(len(sublist)) for sublist in text_ID]\n",
        "doc_topic_count = np.zeros((D, K))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_isNZXZ6kB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate word-topic count matrix with randomly assigned topics\n",
        "\n",
        "for d in range(D):\n",
        "  for w in range(len(text_ID[d])):\n",
        "    topic_doc_assign[d][w] = np.random.choice(K, 1)\n",
        "    # topic\n",
        "    k = int(topic_doc_assign[d][w])\n",
        "    w_id = text_ID[d][w]\n",
        "    \n",
        "    word_topic_count[k][w_id] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcsS6SkA9Di9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cc3ec60d-d344-4635-cf93-650bca25711c"
      },
      "source": [
        "word_topic_count"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 0., 1., ..., 0., 1., 0.],\n",
              "       [2., 1., 1., ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD53tsZq9Gvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find out document-topic count matrix based on above topics\n",
        "\n",
        "for d in range(D):\n",
        "  k_vector = topic_doc_assign[d]\n",
        "  for k in range(K):\n",
        "    doc_topic_count[d][k] = sum(k_vector == k)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bMdjH_E_yv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7fb7eaf8-3fb7-4334-9f94-da9a8dd37b27"
      },
      "source": [
        "doc_topic_count[0:5]"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  4.],\n",
              "       [ 4.,  2.],\n",
              "       [ 8., 14.],\n",
              "       [ 0.,  1.],\n",
              "       [ 1.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPXoKJaqGehW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dfcee40e-9182-4621-e0ab-4e1b4d9dda13"
      },
      "source": [
        "#np.array([doc_topic_count[d][k] for k in range(K)] ) + alpha\n",
        "\n",
        "np.array([word_topic_count[k][197] + beta for k in range(K)])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.001, 5.001])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGXRKYrAAdD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for itr in range(corpus_itter):\n",
        "  for d in range(D):\n",
        "    for w in range(len(text_ID[d])):\n",
        "      \n",
        "      prev_topic_assign = int(topic_doc_assign[d][w])\n",
        "      \n",
        "      w_id = text_ID[d][w]\n",
        "            \n",
        "      doc_topic_count[d][prev_topic_assign] -= 1\n",
        "      word_topic_count[prev_topic_assign][w_id] -= 1\n",
        "      \n",
        "      # Denominator in first term (Numb. of words in doc + numb. topics * alpha)\n",
        "      denom1 = sum(doc_topic_count[d]) + K*alpha # doc_topic_count[0]->[ 3.,  1.] denom1=4.4\n",
        "      \n",
        "      # Denominator in second term (Numb. of words in topic + numb. words in vocab * beta)\n",
        "      denom2 = np.sum(word_topic_count, axis = 1) + V*beta # output : array([9312.393, 9217.393])\n",
        "      \n",
        "      # Numerators, number of words assigned to a topic + prior dirichlet param\n",
        "      numerator1 = np.array([doc_topic_count[d][k]+alpha for k in range(K)])\n",
        "      \n",
        "      numerator2 = np.array([word_topic_count[k][w_id] + beta for k in range(K)])\n",
        "      \n",
        "      prob_topics = (numerator1/denom1)*(numerator2/denom2)\n",
        "      prob_topics = prob_topics/sum(prob_topics)\n",
        "      \n",
        "      update_topic_assign = np.random.choice(K,1,list(prob_topics))\n",
        "      topic_doc_assign[d][w] = update_topic_assign\n",
        "      \n",
        "      # Add in current word back into count matrixes\n",
        "      \n",
        "      doc_topic_count[d][update_topic_assign[0]] += 1\n",
        "      word_topic_count[update_topic_assign[0]][w_id] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_zn9PG_JEiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta = (doc_topic_count + alpha)\n",
        "theta_sum = np.sum(theta, axis=1) # row wise sum\n",
        "theta = theta/theta_sum.reshape((D,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dITJ0ibKJuyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9697925a-7549-49b3-c6d4-6960b20d35d4"
      },
      "source": [
        "# Print document-topic mixture\n",
        "print('Subset of document-topic mixture matrix: \\n%s' % theta[3:6])\n",
        "\n",
        "# Spam comment\n",
        "print( 'Comment {0},\\n topic distribution {1}'.format(\" \".join(text_data[12]), theta[12]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subset of document-topic mixture matrix: \n",
            "[[0.14285714 0.85714286]\n",
            " [0.5        0.5       ]\n",
            " [0.2972973  0.7027027 ]]\n",
            "Comment alright ladi like song check john rage smoke hot rapper come game better eminem lyric hotter hear song channel,\n",
            " topic distribution [0.5257732 0.4742268]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjbEJVoBKquM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2829f137-93c0-47bd-d64c-562d5f27ba37"
      },
      "source": [
        "print( 'Comment {0},\\n topic distribution {1}'.format(\" \".join(text_data[15]), theta[15]))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment href http www facebook com group http www facebook com group,\n",
            " topic distribution [0.71929825 0.28070175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuDr_u6ULAsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute posterior mean of word-topic distribution within documents\n",
        "phi = (word_topic_count + beta)\n",
        "phi_row_sum = np.sum(phi, axis = 1)\n",
        "phi = phi/phi_row_sum.reshape((K,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHueI1QL_I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore the top words that make up each topic \n",
        "\n",
        "# Initialize list of dictionaries\n",
        "\n",
        "list_dict_topics = []\n",
        "\n",
        "# Loop over topics\n",
        "for topic in range(K):\n",
        "    \n",
        "    # Initialize (vocab,prob) dictionary\n",
        "    mydict = {}\n",
        "    \n",
        "    # Loop over vocabular\n",
        "    for word in range(V):\n",
        "        \n",
        "        # Create dictionary {(vocab,prob)}\n",
        "        mydict[list(vocab_total)[word]] = phi[topic][word]\n",
        "        \n",
        "    # Create list of dictionaries\n",
        "    list_dict_topics.append(mydict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avpoi2HbMI6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "78106a8a-32d0-470b-ed4d-5cfdf0dfdb3e"
      },
      "source": [
        "sorted([(value,key) for (key,value) in list_dict_topics[0].items()])[::-1][10:30]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.011168020937260702, 'pleas'),\n",
              " (0.010738485800588529, 'channel'),\n",
              " (0.0086981939013957, 'music'),\n",
              " (0.007302204707211133, 'make'),\n",
              " (0.0069800533547070024, 'view'),\n",
              " (0.006872669570538959, 'www'),\n",
              " (0.006765285786370914, 'new'),\n",
              " (0.006657902002202871, 'get'),\n",
              " (0.006657902002202871, 'amp'),\n",
              " (0.006443134433866783, 'guy'),\n",
              " (0.006013599297194609, 'thank'),\n",
              " (0.005369296592186348, 'watch'),\n",
              " (0.005047145239682217, 'kati'),\n",
              " (0.004939761455514173, 'comment'),\n",
              " (0.004939761455514173, 'best'),\n",
              " (0.004832377671346129, 'quot'),\n",
              " (0.004724993887178086, 'money'),\n",
              " (0.004617610103010043, 'peopl'),\n",
              " (0.004188074966337868, 'year'),\n",
              " (0.004188074966337868, 'know')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj_TXWZjMJs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9fe942ac-aca6-4268-a5fc-4272537e020c"
      },
      "source": [
        "sorted([(value,key) for (key,value) in list_dict_topics[1].items()])[::-1][10:30]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.012368030743616986, 'love'),\n",
              " (0.010849163098503015, 'channel'),\n",
              " (0.008679352176911629, 'thank'),\n",
              " (0.00835388053867292, 'guy'),\n",
              " (0.00824538999259335, 'music'),\n",
              " (0.008136899446513782, 'get'),\n",
              " (0.008028408900434212, 'amp'),\n",
              " (0.007594446716115935, 'make'),\n",
              " (0.007485956170036365, 'view'),\n",
              " (0.007051993985718088, 'www'),\n",
              " (0.00661803180139981, 'comment'),\n",
              " (0.0064010507092406715, 'watch'),\n",
              " (0.0064010507092406715, 'new'),\n",
              " (0.005316145248444978, 'hey'),\n",
              " (0.005099164156285839, 'money'),\n",
              " (0.0048821830641267005, 'quot'),\n",
              " (0.0048821830641267005, 'peopl'),\n",
              " (0.004448220879808422, 'go'),\n",
              " (0.004231239787649284, 'facebook'),\n",
              " (0.004122749241569715, 'see')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT8WD8vXMOMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}